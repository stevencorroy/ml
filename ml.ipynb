{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of flight delay in air transportation using machine learning\n",
    "Steven Corroy\n",
    "\n",
    "The goal of this code is to demonstrate how flight delay can be analyzed and predicted using machine learning.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I) Libraries, data and train/test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A) Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.feature_selection import chi2, f_classif, SelectKBest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import log_loss, roc_auc_score, roc_curve\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_selection import RFE, RFECV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import PredefinedSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root_directory = '/Users/estcorr/data/research/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B) Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the 'On-Time Performance' table from transtats.bts.gov for the 12 months of 2015. There is one file per month as 1.csv for January, 2.csv for Februray, ...\n",
    "\n",
    "This data essentially contains all the flights for the major US carriers during 2015 and their corresponding delay performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/IPython/core/interactiveshell.py:2871: DtypeWarning: Columns (77,84,85) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/Library/Python/2.7/site-packages/IPython/core/interactiveshell.py:2871: DtypeWarning: Columns (77,84) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "delay_list = list()\n",
    "for i in range(1,3):\n",
    "    delay_list.append(pd.read_csv(root_directory+str(i)+'.csv', dtype={'FL_DATE': str,\n",
    "                                                                       'CRS_DEP_TIME': str, \n",
    "                                                                       'DEP_TIME': str, \n",
    "                                                                       'CRS_ARR_TIME': str, \n",
    "                                                                       'ARR_TIME': str}))\n",
    "delay_all = pd.concat(delay_list)\n",
    "delay_all.sort_values(by=['MONTH', 'DAY_OF_MONTH'], inplace=True)\n",
    "delay_all.index = np.arange(len(delay_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the 'T-100 Domestic Segment (U.S. Carriers)' table from transtats.bts.gov for 2015. This table contains monthly data about the amount of passengers transported by carriers on all US routes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pass_list = list()\n",
    "for i in range(1,3):\n",
    "    pass_list.append(pd.read_csv(root_directory+'passengers_'+str(i)+'.csv'))\n",
    "passengers = pd.concat(pass_list)\n",
    "passengers.index = np.arange(len(passengers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we load the excel document provided for the assignment containing the latitude and longitude of all airports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "airports = pd.read_csv(root_directory+'airports2.csv',sep=';')\n",
    "airports.long = airports.long.map(lambda x: x.replace(',','.')).astype('float')\n",
    "airports.lat = airports.lat.map(lambda x: x.replace(',','.')).astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C) Train/test split\n",
    "This is one the delicate things working with time series or time dependant data. We need to split our data into train/validation and test to be able to tune and evaluate our machine learning models. A typical good way to do it for non-time-dependant would be a random 2/3 train and 1/3 test followed a randomized 10-folds cross-validation of the training set for hyper-parameter tunning. Here we cannot do that as it would introduce leakage in the data process, aka we learn from the future. For the sake of time we make a simple split as follows.\n",
    "- The month of January will be used a training set\n",
    "- The first 14 days of February will be used a cross-validation set\n",
    "- The last 14 days of February will be used as test set\n",
    "General statistics used as feature will only be calculated on the training set and daily dynamics features are calculated for all 3 sets.\n",
    "\n",
    "NOTE: a training/learning on many months/years would surely provide more interesting/trustable results. The choice of taking only 2 months into account is purely to be able to run the code in a reasonable amount of time. Simply by changing the data loading part of this code, one could re-run the analysis on a larger dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "delay = delay_all[delay_all.MONTH == 1]\n",
    "delay_cv = delay_all[(delay_all.MONTH == 2) & (delay_all.DAY_OF_MONTH <= 14)]\n",
    "delay_test = delay_all[(delay_all.MONTH == 2) & (delay_all.DAY_OF_MONTH > 14)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accordingly for the passenger statistics we select only January"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "passengers = passengers[passengers.MONTH == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we extract the number of days present in the training data. This will be used all along the code to normalize metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_days = len(np.unique(delay['FL_DATE']))\n",
    "n_days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II) Features generation\n",
    "Here we generate all the features that will be used for predicting delay.\n",
    "\n",
    "GENERAL NOTE FOR THIS NOTEBOOK: many of these features could be computed together in one line (we could decrease the computation time by not repeating 'groupby' and 'join' operations), or in a more efficient/concise way. Here I prefered to disjoin every features to improve readability and for time limitation reason to use function more because the ease of use rather than efficiency (as long as running time was acceptable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = delay[['UNIQUE_CARRIER', 'ORIGIN_AIRPORT_ID', 'DEST_AIRPORT_ID', 'ORIGIN', 'DEST']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A) Airport features\n",
    "These features are related to how the departing and arriving airports behave in general (i.e., for all routes and carriers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Number of flights per day at the departing airport\n",
    "This is the average utilization of the departing airport and could indicate how congested it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dep_flights = (delay.groupby('ORIGIN_AIRPORT_ID').MONTH.count()/n_days).to_frame('DEP_FLIGHTS')\n",
    "data = pd.merge(data, dep_flights, how='inner', left_on='ORIGIN_AIRPORT_ID', right_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Number of flights per day at the arriving airport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arr_flights = (delay.groupby('DEST_AIRPORT_ID').MONTH.count()/n_days).to_frame('ARR_FLIGHTS')\n",
    "data = pd.merge(data, arr_flights, how='inner', left_on='DEST_AIRPORT_ID', right_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Number of routes served by the departing airport\n",
    "This gives how many different routes are offered from the airport and could indicate a large spread of risk in terms of delay but also a more challenging diversity of partners to deal with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dep_routes = (delay.groupby('ORIGIN_AIRPORT_ID').DEST_AIRPORT_ID.agg(lambda x: len(np.unique(x)))).to_frame('DEP_ROUTES')\n",
    "data = pd.merge(data, dep_routes, how='inner', left_on='ORIGIN_AIRPORT_ID', right_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Number of routes ending at the arriving airport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arr_routes = (delay.groupby('DEST_AIRPORT_ID').ORIGIN_AIRPORT_ID.agg(lambda x: len(np.unique(x)))).to_frame('ARR_ROUTES')\n",
    "data = pd.merge(data, arr_routes, how='inner', left_on='DEST_AIRPORT_ID', right_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Number of passengers per day at the departing airport\n",
    "Similar to 1) but taking more into account the flow of passengers rather than planes. To get this one we need to merge with the 'passsengers' table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dep_passengers = (passengers.groupby('ORIGIN_AIRPORT_ID').PASSENGERS.sum()/n_days).to_frame('DEP_PASSENGERS')\n",
    "data = pd.merge(data, dep_passengers, how='inner', left_on='ORIGIN_AIRPORT_ID', right_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Number of passengers per day at the arriving airport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arr_passengers = (passengers.groupby('DEST_AIRPORT_ID').PASSENGERS.sum()/n_days).to_frame('ARR_PASSENGERS')\n",
    "data = pd.merge(data, arr_passengers, how='inner', left_on='DEST_AIRPORT_ID', right_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Rate of occupancy of the departing airport\n",
    "With this we calculate how close to its maximal capacity the airport is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "passengers['OCCUPANCY'] = passengers.PASSENGERS/passengers.SEATS\n",
    "dep_occupancy = (passengers.groupby('ORIGIN_AIRPORT_ID').OCCUPANCY.mean()).to_frame('DEP_OCCUPANCY')\n",
    "data = pd.merge(data, dep_occupancy, how='inner', left_on='ORIGIN_AIRPORT_ID', right_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8) Rate of occupancy of the arriving airport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arr_occupancy = (passengers.groupby('DEST_AIRPORT_ID').OCCUPANCY.mean()).to_frame('ARR_OCCUPANCY')\n",
    "data = pd.merge(data, arr_occupancy, how='inner', left_on='DEST_AIRPORT_ID', right_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B) Route features\n",
    "These features describe the behavior of the specific route considered in general (i.e., for all carriers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Number of times the route is served per day\n",
    "This could indicate if this route is usual for the two airports and is a well-known relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "route_frequency = (delay.groupby(['ORIGIN_AIRPORT_ID','DEST_AIRPORT_ID']).MONTH.count()/n_days).to_frame('ROUTE_FREQ')\n",
    "data = pd.merge(data, route_frequency, how='inner', left_on=['ORIGIN_AIRPORT_ID','DEST_AIRPORT_ID'], right_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Route importance for departing and arriving airports\n",
    "This is to estimate how much percentage of the resources of the airport go to this route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['ROUTE_IMP_DEP'] = data.ROUTE_FREQ/data.DEP_FLIGHTS\n",
    "data['ROUTE_IMP_ARR'] = data.ROUTE_FREQ/data.ARR_FLIGHTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Number of passengers on the route\n",
    "This indicates how busy the route is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "route_passengers = (passengers.groupby(['ORIGIN_AIRPORT_ID','DEST_AIRPORT_ID']).PASSENGERS.sum()/n_days).to_frame('ROUTE_PASSENGERS')\n",
    "data = pd.merge(data, route_passengers, how='inner', left_on=['ORIGIN_AIRPORT_ID','DEST_AIRPORT_ID'], right_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Route importance in terms of passengers\n",
    "This is again to understand how dominating a route is for an airport but this time in terms of number of passengers. We try to catch the fact that taking care of one huge aircraft with many passengers could more work than many smaller aircrafts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['ROUTE_IMP_PASS_DEP'] = data.ROUTE_PASSENGERS/data.DEP_PASSENGERS\n",
    "data['ROUTE_IMP_PASS_ARR'] = data.ROUTE_PASSENGERS/data.ARR_PASSENGERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Route rate of occupancy\n",
    "Here we calculate the average rate of filling of the route. This could indicate how close to maximum capacity the airport is when it comes to this route. Also it shows how much more crowded a flight can be compared to average for the airport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "route_occupancy= (passengers.groupby(['ORIGIN_AIRPORT_ID','DEST_AIRPORT_ID']).OCCUPANCY.mean()).to_frame('ROUTE_OCCUPANCY')\n",
    "data = pd.merge(data, route_occupancy, how='inner', left_on=['ORIGIN_AIRPORT_ID','DEST_AIRPORT_ID'], right_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C) Carrier features\n",
    "These features describe the behavior of the specific carrier considered in general (i.e., for all routes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Number of flights by the carrier at the departing and arriving airports\n",
    "This can show how busy a carrier is in terms of number of flights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dep_carrier_flights = (delay.groupby(['ORIGIN_AIRPORT_ID','UNIQUE_CARRIER']).MONTH.count()/n_days).to_frame('DEP_CARRIER_FLIGHTS')\n",
    "data = pd.merge(data, dep_carrier_flights, how='inner', left_on=['ORIGIN_AIRPORT_ID','UNIQUE_CARRIER'], right_index=True, sort=False)\n",
    "arr_carrier_flights = (delay.groupby(['DEST_AIRPORT_ID','UNIQUE_CARRIER']).MONTH.count()/n_days).to_frame('ARR_CARRIER_FLIGHTS')\n",
    "data = pd.merge(data, arr_carrier_flights, how='inner', left_on=['DEST_AIRPORT_ID','UNIQUE_CARRIER'], right_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Number of routes served by the carrier at the departing and arriving airports\n",
    "This can show how busy a carrier is in terms of number of different routes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dep_carrier_routes = (delay.groupby(['ORIGIN_AIRPORT_ID','UNIQUE_CARRIER']).DEST_AIRPORT_ID.agg(lambda x: len(np.unique(x)))).to_frame('DEP_CARRIER_ROUTES')\n",
    "data = pd.merge(data, dep_carrier_routes, how='inner', left_on=['ORIGIN_AIRPORT_ID','UNIQUE_CARRIER'], right_index=True, sort=False)\n",
    "arr_carrier_routes = (delay.groupby(['DEST_AIRPORT_ID','UNIQUE_CARRIER']).ORIGIN_AIRPORT_ID.agg(lambda x: len(np.unique(x)))).to_frame('ARR_CARRIER_ROUTES')\n",
    "data = pd.merge(data, arr_carrier_routes, how='inner', left_on=['DEST_AIRPORT_ID','UNIQUE_CARRIER'], right_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Number of passengers transported by the carrier at the departing and arriving airports\n",
    "This can show how busy a carrier is in terms of passengers transported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dep_carrier_passengers = (passengers.groupby(['ORIGIN_AIRPORT_ID','UNIQUE_CARRIER']).PASSENGERS.sum()/n_days).to_frame('DEP_CARRIER_PASSENGERS')\n",
    "data = pd.merge(data, dep_carrier_passengers, how='inner', left_on=['ORIGIN_AIRPORT_ID','UNIQUE_CARRIER'], right_index=True, sort=False)\n",
    "arr_carrier_passengers = (passengers.groupby(['DEST_AIRPORT_ID','UNIQUE_CARRIER']).PASSENGERS.sum()/n_days).to_frame('ARR_CARRIER_PASSENGERS')\n",
    "data = pd.merge(data, arr_carrier_passengers, how='inner', left_on=['DEST_AIRPORT_ID','UNIQUE_CARRIER'], right_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Carrier importance\n",
    "Here we estimate how much of the traffic is to account for a carrier at the arriving and receiving airport in terms of flights, routes and passengers transported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['CARRIER_IMP_DEP'] = data.DEP_CARRIER_FLIGHTS/data.DEP_FLIGHTS\n",
    "data['CARRIER_IMP_ROUTE_DEP'] = data.DEP_CARRIER_ROUTES/data.DEP_ROUTES\n",
    "data['CARRIER_IMP_PASS_DEP'] = data.DEP_CARRIER_PASSENGERS/data.DEP_PASSENGERS\n",
    "data['CARRIER_IMP_ARR'] = data.ARR_CARRIER_FLIGHTS/data.ARR_FLIGHTS\n",
    "data['CARRIER_IMP_ROUTE_ARR'] = data.ARR_CARRIER_ROUTES/data.ARR_ROUTES\n",
    "data['CARRIER_IMP_PASS_ARR'] = data.ARR_CARRIER_PASSENGERS/data.ARR_PASSENGERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Rate of occupancy for all the flights of a carrier at the departing and arriving airport\n",
    "This measure should account for the activity of a carrier at an airport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dep_carrier_occupancy = (passengers.groupby(['ORIGIN_AIRPORT_ID','UNIQUE_CARRIER']).OCCUPANCY.mean()).to_frame('DEP_CARRIER_OCCUPANCY')\n",
    "data = pd.merge(data, dep_carrier_occupancy, how='inner', left_on=['ORIGIN_AIRPORT_ID','UNIQUE_CARRIER'], right_index=True, sort=False)\n",
    "arr_carrier_occupancy = (passengers.groupby(['DEST_AIRPORT_ID','UNIQUE_CARRIER']).OCCUPANCY.mean()).to_frame('ARR_CARRIER_OCCUPANCY')\n",
    "data = pd.merge(data, arr_carrier_occupancy, how='inner', left_on=['DEST_AIRPORT_ID','UNIQUE_CARRIER'], right_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D) Carrier and route features\n",
    "These features relate to the performance of a specific carrier on a specific route"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Number of time that the carrier flies the route per day\n",
    "This can describe how regular a route is for a carrier, which could mean being more used to fly the route but also more prone to delay avalanche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "route_carrier_frequency = (delay.groupby(['ORIGIN_AIRPORT_ID','DEST_AIRPORT_ID','UNIQUE_CARRIER']).MONTH.count()/n_days).to_frame('ROUTE_CARRIER_FREQ')\n",
    "data = pd.merge(data, route_carrier_frequency, how='inner', left_on=['ORIGIN_AIRPORT_ID','DEST_AIRPORT_ID','UNIQUE_CARRIER'], right_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Number of passengers per day transported by the carrier on the route\n",
    "How busy the specific route is for the carrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "route_carrier_passengers = (passengers.groupby(['ORIGIN_AIRPORT_ID','DEST_AIRPORT_ID','UNIQUE_CARRIER']).PASSENGERS.sum()/n_days).to_frame('ROUTE_CARRIER_PASSENGERS')\n",
    "data = pd.merge(data, route_carrier_passengers, how='inner', left_on=['ORIGIN_AIRPORT_ID','DEST_AIRPORT_ID','UNIQUE_CARRIER'], right_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Rate of occupancy of the route for the carrier\n",
    "How full the specific route is for the carrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "route_carrier_occupancy= (passengers.groupby(['ORIGIN_AIRPORT_ID','DEST_AIRPORT_ID','UNIQUE_CARRIER']).OCCUPANCY.mean()).to_frame('ROUTE_CARRIER_OCCUPANCY')\n",
    "data = pd.merge(data, route_carrier_occupancy, how='inner', left_on=['ORIGIN_AIRPORT_ID','DEST_AIRPORT_ID','UNIQUE_CARRIER'], right_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Importance of the route for the carrier wrt. number of passengers and number \n",
    "Here we not only compare to the average with the whole airport but also for the route in general and the carrier in general "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['ROUTE_CARRIER_IMP_DEP'] = data.ROUTE_CARRIER_FREQ/data.DEP_FLIGHTS\n",
    "data['ROUTE_CARRIER_IMP_ARR'] = data.ROUTE_CARRIER_FREQ/data.ARR_FLIGHTS\n",
    "data['ROUTE_CARRIER_IMP_PASS_DEP'] = data.ROUTE_CARRIER_PASSENGERS/data.DEP_PASSENGERS\n",
    "data['ROUTE_CARRIER_IMP_PASS_ARR'] = data.ROUTE_CARRIER_PASSENGERS/data.ARR_PASSENGERS\n",
    "\n",
    "data['ROUTE_CARRIER_ROUTE_IMP'] = data.ROUTE_CARRIER_FREQ/data.ROUTE_FREQ\n",
    "data['ROUTE_CARRIER_ROUTE_IMP_PASS'] = data.ROUTE_CARRIER_PASSENGERS/data.ROUTE_PASSENGERS\n",
    "\n",
    "data['ROUTE_CARRIER_CARRIER_IMP_DEP'] = data.ROUTE_CARRIER_FREQ/data.DEP_CARRIER_FLIGHTS\n",
    "data['ROUTE_CARRIER_CARRIER_IMP_ARR'] = data.ROUTE_CARRIER_FREQ/data.ARR_CARRIER_FLIGHTS\n",
    "data['ROUTE_CARRIER_CARRIER_IMP_PASS_DEP'] = data.ROUTE_CARRIER_PASSENGERS/data.DEP_CARRIER_PASSENGERS\n",
    "data['ROUTE_CARRIER_CARRIER_IMP_PASS_ARR'] = data.ROUTE_CARRIER_PASSENGERS/data.ARR_CARRIER_PASSENGERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E) Geography\n",
    "Here we capture the localization of the 2 airports on a route to catch typical weather effect depending on geography. We get long and lat from one of the 2 provided table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.merge(data, airports[['iata','lat','long']], how='inner', left_on='ORIGIN', right_on='iata', sort=False)\n",
    "data = pd.merge(data, airports[['iata','lat','long']], how='inner', left_on='DEST', right_on='iata', sort=False)\n",
    "data.drop(['iata_x','iata_y','ORIGIN','DEST'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F) Putting general statistics together in CV and test sets\n",
    "All the above features have been calculated on the training set and will be reused as such for the CV and test sets. We simply join the CV and test set with the obtained features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_cv = delay_cv[['UNIQUE_CARRIER', 'ORIGIN_AIRPORT_ID', 'DEST_AIRPORT_ID']]\n",
    "data_test = delay_test[['UNIQUE_CARRIER', 'ORIGIN_AIRPORT_ID', 'DEST_AIRPORT_ID']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "general_features = data.groupby(['UNIQUE_CARRIER', 'ORIGIN_AIRPORT_ID', 'DEST_AIRPORT_ID']).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_cv = pd.merge(data_cv, general_features, how='inner', left_on=['UNIQUE_CARRIER', 'ORIGIN_AIRPORT_ID', 'DEST_AIRPORT_ID'], right_index=True, sort=False)\n",
    "data_test = pd.merge(data_test, general_features, how='inner', left_on=['UNIQUE_CARRIER', 'ORIGIN_AIRPORT_ID', 'DEST_AIRPORT_ID'], right_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set size= 469397 , cv set size= 210405 , test set size= 216414\n",
      "(896216, 49)\n"
     ]
    }
   ],
   "source": [
    "n_train_samples = data.shape[0]\n",
    "n_cv_samples = data_cv.shape[0]\n",
    "n_test_samples = data_test.shape[0]\n",
    "\n",
    "print 'training set size=',n_train_samples,', cv set size=',n_cv_samples,', test set size=',n_test_samples\n",
    "\n",
    "data = pd.concat([data, data_cv, data_test])\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F) Time (Periodic effects)\n",
    "Here we capture the general time periodicity with the day of the year and the day of the month. That should help catch typical busy days and typical cold/windy months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['DAY'] = delay_all.DAY_OF_WEEK\n",
    "data['MONTH'] = delay_all.MONTH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## G) Daily time dynamic\n",
    "This is the hard part. Here we want to capture the effect of delay building up gradually, e.g., in an airport under bad weather condition or strike, or for a carrier that has slight delay in the beginning of the day but tight margins and therefore higher and higher delay as time passes. We will gather data for the same day, for things happening 2 hours before the planned departure time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to parse the time in a good way to manipulate it efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "time_columns = ['CRS_DEP_TIME', 'DEP_TIME', 'CRS_ARR_TIME', 'ARR_TIME']\n",
    "for c in time_columns:\n",
    "    delay_all[c+'_CLEAN'] = delay_all[c].apply(lambda x: '0000' if x=='2400' else x)\n",
    "    delay_all[c+'_CLEAN'] = delay_all.apply(lambda x: np.nan if (type(x[c+'_CLEAN']) is not str) or (type(x.FL_DATE) is not str) else x.FL_DATE+'_'+x[c+'_CLEAN'], axis=1)\n",
    "    delay_all[c+'_CLEAN'] = delay_all[c+'_CLEAN'].apply(lambda x: x if (type(x) is not str) else datetime.strptime(x, '%Y-%m-%d_%H%M'))\n",
    "\n",
    "delay_all['CRS_ARR_TIME_CLEAN'] = delay_all.apply(lambda x: x.CRS_ARR_TIME_CLEAN if x.CRS_ARR_TIME_CLEAN > x.CRS_DEP_TIME_CLEAN else x.CRS_ARR_TIME_CLEAN+timedelta(days=1), axis=1)\n",
    "delay_all['ARR_TIME_CLEAN'] = delay_all.apply(lambda x: x.ARR_TIME_CLEAN if x.ARR_TIME_CLEAN > x.DEP_TIME_CLEAN else x.CRS_ARR_TIME_CLEAN+timedelta(days=1), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For each line we want to gather all events happening at the departure or destination airport 2h before the actual CRS departure time, aggregated over the last hour and half day. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For that we create a new key for the flights that will be considered in the 1h and 12h statistics. The idea is that for accelarating computation, we will synchronize different time periods, compute features for these periods and allocate a given period for delay prediction of a given flight. For example if we want to predict a flight on 2015.07.08 at 16:00 we will use the statistics of 2015.07.08 at 14:00 (1h hour period 2h before the flight) and 2015.07.08 morning (hour between 0 and 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def half_day_before_departue(x):\n",
    "    if (x.hour >= 0) and (x.hour<2):\n",
    "        return x.replace(hour=0, minute=0, second=0)-timedelta(hours=24)\n",
    "    elif (x.hour>=2) and (x.hour<12):\n",
    "        return x.replace(hour=0, minute=0, second=0)-timedelta(hours=12)\n",
    "    elif (x.hour >= 12) and (x.hour<14):\n",
    "        return x.replace(hour=12, minute=0, second=0)-timedelta(hours=24)\n",
    "    elif (x.hour>=14) and (x.hour<24):\n",
    "        return x.replace(hour=0, minute=0, second=0)-timedelta(hours=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "delay_all['HOUR_OF_ARR'] = delay_all.ARR_TIME_CLEAN.apply(lambda x: x.replace(minute=0, second=0))\n",
    "delay_all['12H_OF_ARR'] = delay_all.ARR_TIME_CLEAN.apply(lambda x: x.replace(hour=0, minute=0, second=0) if x.hour < 12 else x.replace(hour=12, minute=0, second=0))\n",
    "delay_all['HOUR_BEFORE_DEP'] = delay_all.CRS_DEP_TIME_CLEAN.apply(lambda x: x.replace(minute=0, second=0)-timedelta(hours=2))\n",
    "delay_all['12H_BEFORE_DEP'] = delay_all.CRS_DEP_TIME_CLEAN.apply(lambda x: half_day_before_departue(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['HOUR_BEFORE_DEP'] = delay_all.HOUR_BEFORE_DEP\n",
    "data['12H_BEFORE_DEP'] = delay_all['12H_BEFORE_DEP']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can create our features for those two time periods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Hour of the day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['HOUR_OF_THE_DAY'] = delay_all.CRS_DEP_TIME_CLEAN.map(lambda x: x.hour)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Numbered of delayed flights on the same route and inverse route \n",
    "Here we count how much low,medium and high delay we have during 1h and 12h on both the same route and the inverse route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hour_carrier_route_delay = (delay_all.groupby(['UNIQUE_CARRIER','ORIGIN_AIRPORT_ID','DEST_AIRPORT_ID','HOUR_OF_ARR'])\n",
    "            .ARR_DELAY\n",
    "            .agg({'LOW_DELAY': lambda x: len([elem for elem in x if elem<=5]),\n",
    "                  'MID_DELAY': lambda x: len([elem for elem in x if (elem>5) and (elem <=15)]),\n",
    "                  'HIGH_DELAY': lambda x: len([elem for elem in x if elem>15])}))\n",
    "data = pd.merge(data, hour_carrier_route_delay, how='left', left_on=['UNIQUE_CARRIER','ORIGIN_AIRPORT_ID','DEST_AIRPORT_ID','HOUR_BEFORE_DEP'], right_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['ORIGIN_AIRPORT_ID_R'] = data['DEST_AIRPORT_ID']\n",
    "data['DEST_AIRPORT_ID_R'] = data['ORIGIN_AIRPORT_ID']\n",
    "data = pd.merge(data, hour_carrier_route_delay, how='left', left_on=['UNIQUE_CARRIER','ORIGIN_AIRPORT_ID_R','DEST_AIRPORT_ID_R','HOUR_BEFORE_DEP'], right_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hour12_carrier_route_delay = (delay_all.groupby(['UNIQUE_CARRIER','ORIGIN_AIRPORT_ID','DEST_AIRPORT_ID','12H_OF_ARR'])\n",
    "            .ARR_DELAY\n",
    "            .agg({'LOW_DELAY_12H': lambda x: len([elem for elem in x if elem<=5]),\n",
    "                  'MID_DELAY_12H': lambda x: len([elem for elem in x if (elem>5) and (elem <=15)]),\n",
    "                  'HIGH_DELAY_12H': lambda x: len([elem for elem in x if elem>15])}))\n",
    "data = pd.merge(data, hour12_carrier_route_delay, how='left', left_on=['UNIQUE_CARRIER','ORIGIN_AIRPORT_ID','DEST_AIRPORT_ID','12H_BEFORE_DEP'], right_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.merge(data, hour12_carrier_route_delay, how='left', left_on=['UNIQUE_CARRIER','ORIGIN_AIRPORT_ID_R','DEST_AIRPORT_ID_R','12H_BEFORE_DEP'], right_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Numbered of delayed flights in the departure and destination airport for the carrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hour_carrier_air_delay = (delay_all.groupby(['UNIQUE_CARRIER','ORIGIN_AIRPORT_ID','HOUR_OF_ARR'])\n",
    "            .ARR_DELAY\n",
    "            .agg({'LOW_DELAY_AIR_CAR': lambda x: len([elem for elem in x if elem<=5]),\n",
    "                  'MID_DELAY_AIR_CAR': lambda x: len([elem for elem in x if (elem>5) and (elem <=15)]),\n",
    "                  'HIGH_DELAY_AIR_CAR': lambda x: len([elem for elem in x if elem>15])}))\n",
    "data = pd.merge(data, hour_carrier_air_delay, how='left', left_on=['UNIQUE_CARRIER','ORIGIN_AIRPORT_ID','HOUR_BEFORE_DEP'], right_index=True, sort=False)\n",
    "data = pd.merge(data, hour_carrier_air_delay, how='left', left_on=['UNIQUE_CARRIER','ORIGIN_AIRPORT_ID_R','HOUR_BEFORE_DEP'], right_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hour12_carrier_air_delay = (delay_all.groupby(['UNIQUE_CARRIER','ORIGIN_AIRPORT_ID','12H_OF_ARR'])\n",
    "            .ARR_DELAY\n",
    "            .agg({'LOW_DELAY_AIR_CAR_12H': lambda x: len([elem for elem in x if elem<=5]),\n",
    "                  'MID_DELAY_AIR_CAR_12H': lambda x: len([elem for elem in x if (elem>5) and (elem <=15)]),\n",
    "                  'HIGH_DELAY_AIR_CAR_12H': lambda x: len([elem for elem in x if elem>15])}))\n",
    "data = pd.merge(data, hour12_carrier_air_delay, how='left', left_on=['UNIQUE_CARRIER','ORIGIN_AIRPORT_ID','12H_BEFORE_DEP'], right_index=True, sort=False)\n",
    "data = pd.merge(data, hour12_carrier_air_delay, how='left', left_on=['UNIQUE_CARRIER','ORIGIN_AIRPORT_ID_R','12H_BEFORE_DEP'], right_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Numbered of delayed flights in the departure and destination aiport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hour_air_delay = (delay_all.groupby(['ORIGIN_AIRPORT_ID','HOUR_OF_ARR'])\n",
    "            .ARR_DELAY\n",
    "            .agg({'LOW_DELAY_AIR': lambda x: len([elem for elem in x if elem<=5]),\n",
    "                  'MID_DELAY_AIR': lambda x: len([elem for elem in x if (elem>5) and (elem <=15)]),\n",
    "                  'HIGH_DELAY_AIR': lambda x: len([elem for elem in x if elem>15])}))\n",
    "data = pd.merge(data, hour_air_delay, how='left', left_on=['ORIGIN_AIRPORT_ID','HOUR_BEFORE_DEP'], right_index=True, sort=False)\n",
    "data = pd.merge(data, hour_air_delay, how='left', left_on=['ORIGIN_AIRPORT_ID_R','HOUR_BEFORE_DEP'], right_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hour12_air_delay = (delay_all.groupby(['ORIGIN_AIRPORT_ID','12H_OF_ARR'])\n",
    "            .ARR_DELAY\n",
    "            .agg({'LOW_DELAY_AIR_12H': lambda x: len([elem for elem in x if elem<=5]),\n",
    "                  'MID_DELAY_AIR_12H': lambda x: len([elem for elem in x if (elem>5) and (elem <=15)]),\n",
    "                  'HIGH_DELAY_AIR_12H': lambda x: len([elem for elem in x if elem>15])}))\n",
    "data = pd.merge(data, hour12_air_delay, how='left', left_on=['ORIGIN_AIRPORT_ID','12H_BEFORE_DEP'], right_index=True, sort=False)\n",
    "data = pd.merge(data, hour12_air_delay, how='left', left_on=['ORIGIN_AIRPORT_ID_R','12H_BEFORE_DEP'], right_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(896216, 92)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III) Ground Truth\n",
    "We will solve a multi-class classification problem with 3 types of delay\n",
    "- Low delay: delay $\\le$ 5min\n",
    "- Medium delay: 5min $<$ delay $\\le$ 15min\n",
    "- Large delay: delay $>$ 15min\n",
    "Note that these thresholds are purely arbitrary and based on my personal level of what is low or high delay. This can be easily changed to different values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/IPython/kernel/__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from IPython.kernel.zmq import kernelapp as app\n",
      "/Library/Python/2.7/site-packages/IPython/kernel/__main__.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n",
      "/Library/Python/2.7/site-packages/IPython/kernel/__main__.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "data['y'] = delay_all.ARR_DELAY\n",
    "data.y[data.y <= 5] = 0\n",
    "data.y[data.y > 15] = 2\n",
    "data.y[(data.y != 0) & (data.y != 2) & (~np.isnan(data.y))] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV) Machine learning sets\n",
    "Here we simply get numpy matrices for the training,cv and test sets as well as ground truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we drop information we used for joining tables but we don't want to use for learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.drop(['UNIQUE_CARRIER', 'ORIGIN_AIRPORT_ID', 'DEST_AIRPORT_ID','HOUR_BEFORE_DEP','12H_BEFORE_DEP','ORIGIN_AIRPORT_ID_R','DEST_AIRPORT_ID_R'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we split according to the previously explained time split, fill up the nan values in the features with -1 and remove all samples without groundtruth "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = data.y.values\n",
    "X = data.drop('y',axis=1).fillna(-1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = X[:n_train_samples,:]\n",
    "X_cv = X[n_train_samples:n_train_samples+n_cv_samples,:]\n",
    "X_test = X[n_train_samples+n_cv_samples:,:]\n",
    "\n",
    "y_train = y[:n_train_samples]\n",
    "y_cv = y[n_train_samples:n_train_samples+n_cv_samples]\n",
    "y_test = y[n_train_samples+n_cv_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = X_train[~np.isnan(y_train)]\n",
    "y_train = y_train[~np.isnan(y_train)].astype(int)\n",
    "X_cv = X_cv[~np.isnan(y_cv)]\n",
    "y_cv = y_cv[~np.isnan(y_cv)].astype(int)\n",
    "X_test = X_test[~np.isnan(y_test)]\n",
    "y_test = y_test[~np.isnan(y_test)].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final shapes: train= (456453, 85) , CV= (201103, 85) , test= (204312, 85)\n"
     ]
    }
   ],
   "source": [
    "print 'Final shapes: train=',X_train.shape,', CV=',X_cv.shape,', test=',X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'DEP_FLIGHTS', u'ARR_FLIGHTS', u'DEP_ROUTES', u'ARR_ROUTES',\n",
       "       u'DEP_PASSENGERS', u'ARR_PASSENGERS', u'DEP_OCCUPANCY',\n",
       "       u'ARR_OCCUPANCY', u'ROUTE_FREQ', u'ROUTE_IMP_DEP', u'ROUTE_IMP_ARR',\n",
       "       u'ROUTE_PASSENGERS', u'ROUTE_IMP_PASS_DEP', u'ROUTE_IMP_PASS_ARR',\n",
       "       u'ROUTE_OCCUPANCY', u'DEP_CARRIER_FLIGHTS', u'ARR_CARRIER_FLIGHTS',\n",
       "       u'DEP_CARRIER_ROUTES', u'ARR_CARRIER_ROUTES', u'DEP_CARRIER_PASSENGERS',\n",
       "       u'ARR_CARRIER_PASSENGERS', u'CARRIER_IMP_DEP', u'CARRIER_IMP_ROUTE_DEP',\n",
       "       u'CARRIER_IMP_PASS_DEP', u'CARRIER_IMP_ARR', u'CARRIER_IMP_ROUTE_ARR',\n",
       "       u'CARRIER_IMP_PASS_ARR', u'DEP_CARRIER_OCCUPANCY',\n",
       "       u'ARR_CARRIER_OCCUPANCY', u'ROUTE_CARRIER_FREQ',\n",
       "       u'ROUTE_CARRIER_PASSENGERS', u'ROUTE_CARRIER_OCCUPANCY',\n",
       "       u'ROUTE_CARRIER_IMP_DEP', u'ROUTE_CARRIER_IMP_ARR',\n",
       "       u'ROUTE_CARRIER_IMP_PASS_DEP', u'ROUTE_CARRIER_IMP_PASS_ARR',\n",
       "       u'ROUTE_CARRIER_ROUTE_IMP', u'ROUTE_CARRIER_ROUTE_IMP_PASS',\n",
       "       u'ROUTE_CARRIER_CARRIER_IMP_DEP', u'ROUTE_CARRIER_CARRIER_IMP_ARR',\n",
       "       u'ROUTE_CARRIER_CARRIER_IMP_PASS_DEP',\n",
       "       u'ROUTE_CARRIER_CARRIER_IMP_PASS_ARR', u'lat_x', u'long_x', u'lat_y',\n",
       "       u'long_y', u'DAY', u'MONTH', u'HOUR_OF_THE_DAY', u'LOW_DELAY_x',\n",
       "       u'MID_DELAY_x', u'HIGH_DELAY_x', u'LOW_DELAY_y', u'MID_DELAY_y',\n",
       "       u'HIGH_DELAY_y', u'HIGH_DELAY_12H_x', u'LOW_DELAY_12H_x',\n",
       "       u'MID_DELAY_12H_x', u'HIGH_DELAY_12H_y', u'LOW_DELAY_12H_y',\n",
       "       u'MID_DELAY_12H_y', u'LOW_DELAY_AIR_CAR_x', u'HIGH_DELAY_AIR_CAR_x',\n",
       "       u'MID_DELAY_AIR_CAR_x', u'LOW_DELAY_AIR_CAR_y', u'HIGH_DELAY_AIR_CAR_y',\n",
       "       u'MID_DELAY_AIR_CAR_y', u'MID_DELAY_AIR_CAR_12H_x',\n",
       "       u'HIGH_DELAY_AIR_CAR_12H_x', u'LOW_DELAY_AIR_CAR_12H_x',\n",
       "       u'MID_DELAY_AIR_CAR_12H_y', u'HIGH_DELAY_AIR_CAR_12H_y',\n",
       "       u'LOW_DELAY_AIR_CAR_12H_y', u'LOW_DELAY_AIR_x', u'MID_DELAY_AIR_x',\n",
       "       u'HIGH_DELAY_AIR_x', u'LOW_DELAY_AIR_y', u'MID_DELAY_AIR_y',\n",
       "       u'HIGH_DELAY_AIR_y', u'MID_DELAY_AIR_12H_x', u'LOW_DELAY_AIR_12H_x',\n",
       "       u'HIGH_DELAY_AIR_12H_x', u'MID_DELAY_AIR_12H_y', u'LOW_DELAY_AIR_12H_y',\n",
       "       u'HIGH_DELAY_AIR_12H_y', u'y'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V) Feature selection using ANOVA\n",
    "Use analysis of variance to see which feature best separate the ground truth "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "F,P = f_classif(np.delete(X_train, 47, 1),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'HIGH_DELAY_AIR_y', u'HIGH_DELAY_AIR_CAR_y', u'MID_DELAY_AIR_y',\n",
       "       u'HIGH_DELAY_AIR_12H_y', u'HIGH_DELAY_AIR_CAR_12H_y',\n",
       "       u'MID_DELAY_AIR_12H_y', u'MID_DELAY_AIR_CAR_y', u'HOUR_OF_THE_DAY',\n",
       "       u'HIGH_DELAY_12H_y', u'HIGH_DELAY_AIR_x', u'HIGH_DELAY_12H_x',\n",
       "       u'ARR_FLIGHTS', u'ARR_ROUTES', u'ARR_PASSENGERS',\n",
       "       u'MID_DELAY_AIR_CAR_12H_y', u'ARR_OCCUPANCY', u'LOW_DELAY_AIR_12H_x',\n",
       "       u'HIGH_DELAY_AIR_CAR_x', u'ROUTE_IMP_ARR',\n",
       "       u'ROUTE_CARRIER_CARRIER_IMP_ARR'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns.drop('MONTH')[np.argsort(F)[-20:][::-1]]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Here we only look at general features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "F,P = f_classif(X_train[:,:47],y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'ARR_FLIGHTS', u'ARR_ROUTES', u'ARR_PASSENGERS', u'ARR_OCCUPANCY',\n",
       "       u'ROUTE_IMP_ARR'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns[np.argsort(F)[-5:][::-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "487.76070261867602"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VI) Feature importance using Random Forest\n",
    "In this section, we will look into the first classification accuracy results and which features dominates the classification process in Random Forest to derive which features are more influential on the delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_rf = RandomForestClassifier(n_estimators=100, criterion='gini', max_depth=5, \n",
    "                               min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0,\n",
    "                               max_features='auto', max_leaf_nodes=None, min_impurity_split=1e-07,\n",
    "                               bootstrap=True, oob_score=False, n_jobs=-1, random_state=2016, \n",
    "                               verbose=1, warm_start=False, class_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    4.4s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   11.5s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=5, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=100, n_jobs=-1, oob_score=False,\n",
       "            random_state=2016, verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.3s finished\n"
     ]
    }
   ],
   "source": [
    "y_pred = model_rf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss error= 0.92356565029\n"
     ]
    }
   ],
   "source": [
    "print 'Log loss error=', log_loss(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_same = np.ones((y_pred.shape[0],3))*(1/3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss error= 1.09861228867\n"
     ]
    }
   ],
   "source": [
    "print 'Random guess log loss error=', log_loss(y_test, y_same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([84, 78, 82, 71, 83, 65, 77, 76,  1,  5, 45,  3,  7, 24, 48, 66, 46,\n",
       "       72, 64, 75, 16, 44, 58, 18, 20, 26, 74, 28])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idmax = np.argsort(model_rf.feature_importances_)[-10:][::-1]\n",
    "idmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.11317839,  0.08728532,  0.07348048,  0.0635536 ,  0.05925539,\n",
       "        0.05619052,  0.05057001,  0.04956328,  0.04244013,  0.04237969,\n",
       "        0.04051474,  0.03750147,  0.03099607,  0.02054762,  0.0202518 ,\n",
       "        0.01818879,  0.01723635,  0.01423387,  0.01377152,  0.01358571,\n",
       "        0.01334697,  0.01176555,  0.01159248,  0.01088431,  0.00954504,\n",
       "        0.00682398,  0.00582341,  0.00581226])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rf.feature_importances_[idmax]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'HIGH_DELAY_AIR_12H_y', u'HIGH_DELAY_AIR_y', u'MID_DELAY_AIR_12H_y',\n",
       "       u'HIGH_DELAY_AIR_CAR_12H_y', u'LOW_DELAY_AIR_12H_y',\n",
       "       u'HIGH_DELAY_AIR_CAR_y', u'MID_DELAY_AIR_y', u'LOW_DELAY_AIR_y',\n",
       "       u'ARR_FLIGHTS', u'ARR_PASSENGERS', u'long_y', u'ARR_ROUTES',\n",
       "       u'ARR_OCCUPANCY', u'CARRIER_IMP_ARR', u'HOUR_OF_THE_DAY',\n",
       "       u'MID_DELAY_AIR_CAR_y', u'DAY', u'LOW_DELAY_AIR_CAR_12H_y',\n",
       "       u'LOW_DELAY_AIR_CAR_y', u'HIGH_DELAY_AIR_x', u'ARR_CARRIER_FLIGHTS',\n",
       "       u'lat_y', u'HIGH_DELAY_12H_y', u'ARR_CARRIER_ROUTES',\n",
       "       u'ARR_CARRIER_PASSENGERS', u'CARRIER_IMP_PASS_ARR', u'MID_DELAY_AIR_x',\n",
       "       u'ARR_CARRIER_OCCUPANCY'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns[idmax]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we run random forest only on general feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    6.8s finished\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss error= 0.945074919098\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index([u'ARR_FLIGHTS', u'ARR_PASSENGERS', u'long_y', u'ARR_ROUTES',\n",
       "       u'ARR_OCCUPANCY'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rf.fit(X_train[:,:47], y_train)\n",
    "y_pred = model_rf.predict_proba(X_test[:,:47])\n",
    "print 'Log loss error=', log_loss(y_test, y_pred)\n",
    "data.columns[np.argsort(model_rf.feature_importances_)[-5:][::-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we do the opposite, we run only on daily features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    6.6s finished\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss error= 0.918437564556\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index([u'HIGH_DELAY_AIR_12H_y', u'HIGH_DELAY_AIR_y', u'MID_DELAY_AIR_12H_y',\n",
       "       u'HIGH_DELAY_AIR_CAR_12H_y', u'LOW_DELAY_AIR_12H_y'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rf.fit(X_train[:,49:], y_train)\n",
    "y_pred = model_rf.predict_proba(X_test[:,49:])\n",
    "print 'Log loss error=', log_loss(y_test, y_pred)\n",
    "data.columns[49+np.argsort(model_rf.feature_importances_)[-5:][::-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VII) Recursive feature elimination with cross validation\n",
    "Here we will try to recursively eliminate features to keep only those which provide the best cv score. For that we take a simple fast linear classifier as backbone and run REFCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_sgd = SGDClassifier(loss='hinge', penalty='l2', alpha=0.0001, l1_ratio=0.15, \n",
    "                      fit_intercept=True, n_iter=10, shuffle=True, verbose=0, \n",
    "                      epsilon=0.1, n_jobs=1, random_state=None, learning_rate='optimal', \n",
    "                      eta0=0.0, power_t=0.5, class_weight=None, warm_start=False, average=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RFE(estimator=SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='hinge', n_iter=10, n_jobs=1,\n",
       "       penalty='l2', power_t=0.5, random_state=None, shuffle=True,\n",
       "       verbose=0, warm_start=False),\n",
       "  n_features_to_select=10, step=1, verbose=0)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector_simple = RFE(model_sgd, n_features_to_select=10, step=1, verbose=0)\n",
    "selector_simple.fit(np.vstack([X_train, X_cv]), np.hstack([y_train,y_cv]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'HIGH_DELAY_12H_x', u'LOW_DELAY_12H_x', u'MID_DELAY_AIR_CAR_x',\n",
       "       u'LOW_DELAY_AIR_CAR_y', u'HIGH_DELAY_AIR_CAR_y', u'MID_DELAY_AIR_CAR_y',\n",
       "       u'HIGH_DELAY_AIR_x', u'MID_DELAY_AIR_y', u'HIGH_DELAY_AIR_y',\n",
       "       u'MID_DELAY_AIR_12H_y'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data.columns[np.hstack([selector_simple.support_,np.array([False])])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we run feature elimination only on general features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_sgd = SGDClassifier(loss='hinge', penalty='l2', alpha=0.0001, l1_ratio=0.15, \n",
    "                      fit_intercept=True, n_iter=10, shuffle=True, verbose=0, \n",
    "                      epsilon=0.1, n_jobs=1, random_state=None, learning_rate='optimal', \n",
    "                      eta0=0.0, power_t=0.5, class_weight=None, warm_start=False, average=False)\n",
    "selector_simple_general = RFE(model_sgd, n_features_to_select=10, step=1, verbose=0)\n",
    "selector_simple_general.fit(np.vstack([X_train[:,:47], X_cv[:,:47]]), np.hstack([y_train,y_cv]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'ARR_ROUTES', u'ROUTE_FREQ', u'ARR_CARRIER_FLIGHTS',\n",
       "       u'DEP_CARRIER_ROUTES', u'ARR_CARRIER_ROUTES', u'ARR_CARRIER_PASSENGERS',\n",
       "       u'ROUTE_CARRIER_FREQ', u'lat_x', u'long_x', u'long_y'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns[np.hstack([selector_simple_general.support_,np.array([False]*(X_train.shape[1]-46))])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we run REFCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_sgd = SGDClassifier(loss='hinge', penalty='l2', alpha=0.0001, l1_ratio=0.15, \n",
    "                      fit_intercept=True, n_iter=10, shuffle=True, verbose=0, \n",
    "                      epsilon=0.1, n_jobs=1, random_state=None, learning_rate='optimal', \n",
    "                      eta0=0.0, power_t=0.5, class_weight=None, warm_start=False, average=False)\n",
    "\n",
    "cv_fold = np.ones(X_train.shape[0]+X_cv.shape[0])\n",
    "cv_fold[:X_train.shape[0]]=-1\n",
    "cv = PredefinedSplit(cv_fold)\n",
    "\n",
    "selector = RFECV(model_sgd, step=1, cv=cv, verbose=0, n_jobs=1)\n",
    "selector.fit(np.vstack([X_train, X_cv]), np.hstack([y_train,y_cv]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "selector.n_features_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.columns[np.hstack([selector.support_,np.array([False])])]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
